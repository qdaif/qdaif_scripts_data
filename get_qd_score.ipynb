{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_PATHS = {\n",
    "    \"/nfs/scratch_2/adai/qdhf_sandbox/results/gen_runs_paper_story_fixed/rouge_filter/opinions\": \"LMX, ROUGE-L\",\n",
    "    \"/nfs/scratch_2/adai/qdhf_sandbox/results/gen_runs_paper_story_fixed/rouge_quality_si/opinions\": \"LMX, ROUGE-L (w/ QAIF)\",\n",
    "    \"/nfs/scratch_2/adai/qdhf_sandbox/results/gen_runs_paper_story_fixed/nsaif/opinions\": \"LMX, NSAIF\",\n",
    "    \"/nfs/scratch_2/adai/qdhf_sandbox/results/gen_runs_paper_story_fixed/nsaif_q_filter/opinions\": \"LMX, NSAIF (w/ QAIF)\",\n",
    "    \"/nfs/scratch_2/adai/openelm_no_install/results/data/histories_opinions_stories/qdaif/opinions/lmx_near_seeded_init\": \"QDAIF (ours)\",\n",
    "}\n",
    "N_BINS = 20 # number of bin intervals for 1D\n",
    "ELITES_INTERVALS = 100 # elites at a given archive state saved every N intervals\n",
    "TITLE = \"Stories - Genre and Ending\"\n",
    "Y_LABEL = \"QD Score\" # \"QD Score\", \"Coverage\", \"Best Solution Quality\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, Tuple, Union, List\n",
    "import json\n",
    "from map_elites import MAPElitesEvaluation, MAPElites2DEvaluation\n",
    "\n",
    "def bootstrap_samples(data, n_bootstrap_samples=1000):\n",
    "    \"\"\"Generates bootstrap samples.\"\"\"\n",
    "    n_points = data.shape[1]\n",
    "    bootstrap_samples = np.empty((n_bootstrap_samples, data.shape[0], n_points))\n",
    "    \n",
    "    for i in range(n_bootstrap_samples):\n",
    "        sample_indices = np.random.choice(data.shape[0], size=data.shape[0], replace=True)\n",
    "        bootstrap_samples[i] = data[sample_indices]\n",
    "    \n",
    "    return bootstrap_samples\n",
    "\n",
    "def plot_with_bootstrap_ci(data, run_name, ax, stat_func):\n",
    "    \"\"\"\n",
    "    Plots the statistic along with its 95% CI using bootstrapping.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: 2D array, shape (num_seeds, timesteps), data to plot\n",
    "    - run_name: str, name of the run to be used in the legend\n",
    "    - ax: matplotlib axis, axis to plot on\n",
    "    - stat_func: function, function to compute the statistic\n",
    "    \"\"\"\n",
    "    n_bootstrap_samples = 1000\n",
    "    bootstrap_s = bootstrap_samples(data, n_bootstrap_samples)\n",
    "    \n",
    "    # stats for each bootstrap sample and for each timestep\n",
    "    bootstrap_stat = np.array([stat_func(sample, axis=0) for sample in bootstrap_s])\n",
    "    \n",
    "    # 95% CI\n",
    "    lower_bound = np.percentile(bootstrap_stat, 2.5, axis=0)\n",
    "    upper_bound = np.percentile(bootstrap_stat, 97.5, axis=0)\n",
    "    stat_original = stat_func(data, axis=0)\n",
    "    print(stat_original[-1], lower_bound[-1], upper_bound[-1])\n",
    "\n",
    "    timesteps = np.arange(data.shape[1])\n",
    "    ax.plot(timesteps, stat_original, label=run_name)\n",
    "    ax.fill_between(timesteps, lower_bound, upper_bound, alpha=0.2)\n",
    "    ax.set_xlabel(\"Iterations\", fontsize=14)\n",
    "    ax.set_ylabel(Y_LABEL, fontsize=14)\n",
    "    ax.set_title(TITLE, fontsize=16)\n",
    "    ax.legend(loc=\"lower right\", fontsize=8)\n",
    "\n",
    "def plot_mean(data, run_name, ax):\n",
    "    plot_with_bootstrap_ci(data, run_name, ax, np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_expt_dir = \"\"\n",
    "qd_score_runs = {}\n",
    "dirs = [Path(list(RESULT_PATHS.keys())[i]) for i in range(len(RESULT_PATHS))]\n",
    "\n",
    "# contains every experiment of runs in base_dir\n",
    "for base_dir in dirs:\n",
    "    # sort so that experiments are processed at a time\n",
    "    for sub_dir in sorted(base_dir.rglob(\"history.jsonl\")):\n",
    "        expt_dir = sub_dir.parent.parent # this dir would contain multiple seed runs\n",
    "        # sub_dir goes through all rerun dir elites for an expt dir, so we continue onto next expt dir \n",
    "        if expt_dir == prev_expt_dir:\n",
    "            continue\n",
    "        \n",
    "        # initialize to store stats across reruns in single experiment\n",
    "        qd_score_seeds = []\n",
    "\n",
    "        # process all random rng seed reruns in experiment\n",
    "        for seed_dir in expt_dir.rglob(\"history.jsonl\"):\n",
    "            X = pd.read_json(seed_dir, lines=True)\n",
    "            history_length = len(X) # set it to a shorter length of iterations if desired\n",
    "\n",
    "            elites_path = None\n",
    "\n",
    "            if \"qdef/\" in str(seed_dir): # for embedding feedback experiment logs\n",
    "                custom_bins = np.array([0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.50, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60])\n",
    "            else: # default\n",
    "                custom_bins = np.array([0.005, 0.01, 0.015, 0.02, 0.03, 0.04, 0.05, 0.10, 0.20, 0.50, 0.80, 0.90, 0.95, 0.96, 0.97, 0.98, 0.985, 0.99, 0.995])\n",
    "\n",
    "            if \"stories_genre_ending\" in str(seed_dir): # 2D grid domain\n",
    "                X_BINS = [0.005, 0.02, 0.05, 0.20, 0.50, 0.80, 0.95, 0.98, 0.995]\n",
    "                Y_BINS = [0.005, 0.02, 0.05, 0.20, 0.50, 0.80, 0.95, 0.98, 0.995]\n",
    "\n",
    "                map_elites_evaluation = MAPElites2DEvaluation(history_length=history_length, x_bins=X_BINS, y_bins=Y_BINS, start=(0,0), stop=(1,1), elites_intervals=ELITES_INTERVALS)\n",
    "                map_elites_evaluation.fit(phenotype_key=\"phenotype\", data=X, elites_path=elites_path)\n",
    "            else:\n",
    "                # pass\n",
    "                map_elites_evaluation = MAPElitesEvaluation(history_length=history_length, n_bins=N_BINS, start=0, stop=1, custom_bins=custom_bins, elites_intervals=ELITES_INTERVALS)\n",
    "                map_elites_evaluation.fit(phenotype_key=\"phenotype\", data=X, elites_path=elites_path)\n",
    "\n",
    "            if Y_LABEL == \"QD Score\":\n",
    "                qd_score_seed = map_elites_evaluation.qd_scores[:2000] # cap iterations if loading from files with longer histories (e.g. 5000)\n",
    "            elif Y_LABEL == \"Coverage\":\n",
    "                qd_score_seed = map_elites_evaluation.coverage[:2000]\n",
    "            elif Y_LABEL == \"Best Solution Quality\":\n",
    "                qd_score_seed = map_elites_evaluation.max_fitnesses[:2000]\n",
    "            qd_score_seeds.append([qd_score_seed])\n",
    "        \n",
    "        qd_score_runs[str(expt_dir)] = qd_score_seeds\n",
    "        prev_expt_dir = expt_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "\n",
    "# compute stats for each run/experiment, and add to same plot\n",
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "\n",
    "for run in qd_score_runs.keys():\n",
    "    if run in RESULT_PATHS.keys():\n",
    "        run_data = qd_score_runs[run]\n",
    "        run_name = RESULT_PATHS[run]\n",
    "        run_data = np.array(run_data)[:,0]\n",
    "        plot_mean(run_data, run_name, ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t_andrew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
